{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLjP5E_PloHg"
      },
      "source": [
        "# Sentiment Analysis of Movie Reviews\n",
        "## Data Set Description:\n",
        "IMDB dataset is obtained from Kaggle URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv\n",
        "\n",
        "It has 50K highly polar movie reviews. Each row in the data has a review and a sentiment (positive and negative) about a movie.\n",
        "\n",
        "## Classification Problem:\n",
        "My goal is to build a machine learning classifier that can predict whether a review about a movie is positive or negative based on the review text only. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab Setup for PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ughtCg3uljXF",
        "outputId": "b12c4bc6-4446-4372-d24f-f0421c1292a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 24.2 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --ignore-installed -q pyspark==3.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRtNZ5vAlsS_",
        "outputId": "dddb729a-055f-40d8-c966-0115c86dca60"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import requests\n",
        "from operator import add\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.feature import IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import ChiSqSelector\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Computation of time elapsed\n",
        "# Start time\n",
        "begin_time = time.time()\n",
        "# End time\n",
        "end_time = time.time() - begin_time\n",
        "print(\"Total execution time to learn SVM model on training data: \", end_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pyscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile run_Term_Project.py\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.feature import IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import ChiSqSelector\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "import time\n",
        "\n",
        "# Function to predict the sentiment(positive, negative) of text for different classifiers\n",
        "def getPrediction(text, model):\n",
        "    # Check if text is one review or a list of reviews\n",
        "    if (isinstance(text, str)):\n",
        "        review = [text]\n",
        "    else:\n",
        "        review = text\n",
        "\n",
        "    # Create a dataframe of review list\n",
        "    df_new_data = spark.createDataFrame(review, StringType()) \n",
        "\n",
        "    # Rename the dataframe column\n",
        "    df_new_data = df_new_data.withColumnRenamed(\"value\", \"review\")\n",
        "\n",
        "    # Predict sentiment using the SVM model\n",
        "    predict = model.transform(df_new_data)\n",
        "    predict = predict.select(\"review\", \"prediction\")\n",
        "    return predict\n",
        "\n",
        "# main() starts here\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 2:\n",
        "        print(\"Usage: wordcount <file> <output> \", file=sys.stderr)\n",
        "        exit(-1)\n",
        "\n",
        "    # Create spark context   \n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Sentiment Analysis of IMDB Dataset\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Set your file path here \n",
        "    data_file = \"/Users/munibasiddiqi/Desktop/BUCS777/Homework_Assignments/Term_Project/IMDB_Dataset.csv\"\n",
        "    \n",
        "    # Colab path\n",
        "    #data_file = \"IMDB_Dataset.csv\"\n",
        "\n",
        "    # Google cloud path\n",
        "    #data_file = \"gs://met-cs-777-data/TrainingData.txt.bz2\"\n",
        "\n",
        "    # Upload data\n",
        "    spark_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"escape\",\"\\\"\").option(\"multiLine\",\"true\").load(data_file)\n",
        "\n",
        "    # Add label column to dataframe (Positive sentiment = 1, Negative sentiment = 0)\n",
        "    df = spark_df.withColumn(\"label\", F.when(F.col(\"sentiment\")==\"positive\",1).otherwise(0)).cache()\n",
        "\n",
        "    # Split the dataset into training and test set\n",
        "    df_train, df_test = df.randomSplit(weights=[0.7, 0.3], seed=100)\n",
        "\n",
        "    # Check if the dataset is balanced or imbalanced\n",
        "    print(\"Taining Data\")\n",
        "    df_train.groupby(\"label\").count().show()\n",
        "    print(\"Test Data\")\n",
        "    df_test.groupby(\"label\").count().show()\n",
        "\n",
        "    ############################### Data Preprocessing #######################################\n",
        "    \n",
        "    # Converts text to lowercase and split text on non-word character\n",
        "    regexTokenizer = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "    \n",
        "    # Remove stopwords\n",
        "    remover = StopWordsRemover(inputCol = regexTokenizer.getOutputCol(), outputCol=\"filtered\")\n",
        "\n",
        "    # Remove stopWords=[\"br\", 'm', 've', 're', 'll', 'd']\n",
        "    remover2 = StopWordsRemover(inputCol= remover.getOutputCol(), outputCol=\"token\",stopWords=[\"br\", 'm', 've', 're', 'll', 'd'])\n",
        "\n",
        "    # Extracts a vocabulary from document collections and generates a CountVectorizerModel\n",
        "    # During the fitting process, CountVectorizer will select the top vocabSize words ordered \n",
        "    # by term frequency across the corpus.\n",
        "    countVectorizer = CountVectorizer(inputCol= remover2.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=5000)\n",
        "    \n",
        "    # The IDF Model takes feature vectors and scales each feature. \n",
        "    # Intuitively, it down-weights features which appear frequently in a corpus\n",
        "    idf = IDF(inputCol= countVectorizer.getOutputCol(), outputCol=\"featuresIDF\")\n",
        "\n",
        "    # Chi-Squared feature selection. It operates on labeled data with categorical features. \n",
        "    # ChiSqSelector uses the Chi-Squared test of independence to decide which features to choose. \n",
        "    selector = ChiSqSelector(numTopFeatures=500, featuresCol=idf.getOutputCol(),\n",
        "                         outputCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "\n",
        "############################### LOGISTIC REGRESSION ######################################\n",
        "\n",
        "    ##################### Training Model ####################\n",
        "\n",
        "    # Start time\n",
        "    begin_time = time.time()\n",
        "\n",
        "    # LogisticRegression classifier\n",
        "    classifier_logreg = LogisticRegression(maxIter=20)\n",
        "\n",
        "    # Chain indexers and classifier_logreg in a Pipeline\n",
        "    pipeline_logreg = Pipeline(stages=[regexTokenizer, remover, remover2, countVectorizer, idf, classifier_logreg])\n",
        "    \n",
        "    # Train model. \n",
        "    model_logreg = pipeline_logreg.fit(df_train)\n",
        "\n",
        "    # Print the coefficients and intercept for linear SVC\n",
        "    print(\"Logistic Regression Model\")\n",
        "    print(\"First 10 Coefficients: \" + str(model_logreg.stages[5].coefficients[:10]))\n",
        "    print(\"Intercept: \" + str(model_logreg.stages[5].intercept))\n",
        "\n",
        "    # Top 20 vocabulary words\n",
        "    #pipeline_logreg.getStages()\n",
        "    vocabulary = model_logreg.stages[3].vocabulary\n",
        "    print(\"Top twenty vocabulary words\", vocabulary[0:20])\n",
        "\n",
        "    # End time\n",
        "    end_time = time.time() - begin_time\n",
        "    print(\"Total execution time to train logistic regression model on the train data: \", end_time)\n",
        "   \n",
        "    # Create a dataframe of top 20 vocabulary words to save as csv file\n",
        "    df_top20 = spark.createDataFrame(vocabulary[0:20], StringType())\n",
        "\n",
        "    # Store this result in a single file on the cluster\n",
        "    df_top20.coalesce(1).write.format(\"csv\").option(\"header\",True).save(sys.argv[1]+'.top20_words_IDF')\n",
        "\n",
        "    ##################### Model Testing ####################\n",
        "\n",
        "    # Start time\n",
        "    begin_time = time.time()\n",
        "\n",
        "    # Make predictions.\n",
        "    predictions_logreg = model_logreg.transform(df_test).cache()\n",
        "\n",
        "    # End time\n",
        "    end_time = time.time() - begin_time\n",
        "    print(\"Total execution time to test logistic regression model on the test data: \", end_time)\n",
        "\n",
        "    ##################### Model evaluation ####################\n",
        "\n",
        "    # Start time\n",
        "    begin_time = time.time()\n",
        "    \n",
        "    # Covert dataframe to RDD for Model evaluation\n",
        "    predictionAndLabels_logreg = predictions_logreg.select(\"label\",  \"prediction\").rdd.map(lambda x : (float(x[0]), float(x[1]))).cache()\n",
        "\n",
        "    # Instantiate metrics object\n",
        "    metrics_logreg = MulticlassMetrics(predictionAndLabels_logreg)\n",
        "\n",
        "    # Statistics by class\n",
        "    #labels = data.map(lambda lp: lp.label).distinct().collect()\n",
        "    print(\"Summary statistics for Logistic regression classifier\")\n",
        "    labels = [0.0, 1.0]\n",
        "    for label in sorted(labels):\n",
        "        print(\"Class %s precision = %s\" % (label, metrics_logreg.precision(label)))\n",
        "        print(\"Class %s recall = %s\" % (label, metrics_logreg.recall(label)))\n",
        "        print(\"Class %s F1 Measure = %s\" % (label, metrics_logreg.fMeasure(label, beta=1.0)))\n",
        "    \n",
        "    print(\"Accuracy = %s\" % metrics_logreg.accuracy)\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(metrics_logreg.confusionMatrix().toArray().astype(int))\n",
        "\n",
        "    # End time\n",
        "    end_time = time.time() - begin_time\n",
        "    print(\"Total execution time to evalute the performance of logistic regression model on test data: \", end_time)\n",
        "\n",
        "    # Create a dataframe to store the summary of results\n",
        "    data = [(\"Accuracy\", str(metrics_logreg.accuracy)),(\"Confusion Matrix\",str(metrics_logreg.confusionMatrix().toArray()))]\n",
        "    df = spark.createDataFrame(data)\n",
        "\n",
        "    # Store this result in a single file on the cluster\n",
        "    df.coalesce(1).write.format(\"csv\").option(\"header\",True).save(sys.argv[1]+'.logreg_statistics')\n",
        "\n",
        "\n",
        "\n",
        " ############################### SUPPORT VECTOR MACHINE ######################################\n",
        "\n",
        "    ##################### Training Model ####################\n",
        "\n",
        "    # Start time\n",
        "    begin_time = time.time()\n",
        "\n",
        "    # SVM classifier\n",
        "    classifier_lsvc = LinearSVC(maxIter=20)\n",
        "\n",
        "    # Fit the model\n",
        "    #lsvcModel = classifier.fit(df_train)\n",
        "\n",
        "    # Chain indexers and classifier_lsvc in a Pipeline\n",
        "    pipeline_lsvc = Pipeline(stages=[regexTokenizer, remover, remover2, countVectorizer, idf, classifier_lsvc])\n",
        "\n",
        "    # Train model. \n",
        "    model_lsvc = pipeline_lsvc.fit(df_train)\n",
        "\n",
        "    # Print the coefficients and intercept for linear SVC\n",
        "    print(\"Support Vector Machine Model\")\n",
        "    print(\"First 10 Coefficients: \" + str(model_lsvc.stages[5].coefficients[:10]))\n",
        "    print(\"Intercept: \" + str(model_lsvc.stages[5].intercept))\n",
        "\n",
        "    # End time\n",
        "    end_time = time.time() - begin_time\n",
        "    print(\"Total execution time to train SVM model on the train data: \", end_time)\n",
        "\n",
        "    ##################### Model Testing ####################\n",
        "\n",
        "    # Start time\n",
        "    begin_time = time.time()\n",
        "\n",
        "    # Make predictions.\n",
        "    predictions_lsvc = model_lsvc.transform(df_test).cache()\n",
        "\n",
        "    # End time\n",
        "    end_time = time.time() - begin_time\n",
        "    print(\"Total execution time to test SVM model on the test data: \", end_time)\n",
        "\n",
        "    ##################### Model evaluation ####################\n",
        "\n",
        "    # Start time\n",
        "    begin_time = time.time()\n",
        "    \n",
        "    # Covert dataframe to RDD for Model evaluation\n",
        "    predictionAndLabels_lsvc = predictions_lsvc.select(\"label\",  \"prediction\").rdd.map(lambda x : (float(x[0]), float(x[1]))).cache()\n",
        "\n",
        "    # Instantiate metrics object\n",
        "    metrics_lsvc = MulticlassMetrics(predictionAndLabels_lsvc)\n",
        "\n",
        "\n",
        "    # Statistics by class\n",
        "    #labels = data.map(lambda lp: lp.label).distinct().collect()\n",
        "    labels = [0.0, 1.0]\n",
        "    for label in sorted(labels):\n",
        "        print(\"Class %s precision = %s\" % (label, metrics_lsvc.precision(label)))\n",
        "        print(\"Class %s recall = %s\" % (label, metrics_lsvc.recall(label)))\n",
        "        print(\"Class %s F1 Measure = %s\" % (label, metrics_lsvc.fMeasure(label, beta=1.0)))\n",
        "    \n",
        "    print(\"Accuracy = %s\" % metrics_lsvc.accuracy)\n",
        "    print(metrics_lsvc.confusionMatrix().toArray().astype(int))\n",
        "\n",
        "    # End time\n",
        "    end_time = time.time() - begin_time\n",
        "    print(\"Total execution time to evalute the performance of SVM model on test data: \", end_time)\n",
        "\n",
        "    # Create a dataframe to store the summary of results\n",
        "    data = [(\"Accuracy\", str(metrics_lsvc.accuracy)),(\"Confusion Matrix\",str(metrics_lsvc.confusionMatrix().toArray()))]\n",
        "    df = spark.createDataFrame(data)\n",
        "\n",
        "    # Store this result in a single file on the cluster\n",
        "    df.coalesce(1).write.format(\"csv\").option(\"header\",True).save(sys.argv[1]+'.SVM_statistics')\n",
        "\n",
        "\n",
        "########################## Predicting Sentiments on New Data (Reviews)  ############################\n",
        "\n",
        "   # A list of reviews \n",
        "    new_data = ['This movie was horrible, plot was boring, acting was okay.',\n",
        "                'The film really sucked. I want my money back',\n",
        "                'What a beautiful movie. Great plot, great acting.',\n",
        "                'Harry Potter was a good movie.'\n",
        "                ]\n",
        "\n",
        "    ################# Prediction using logistic regression model ###############\n",
        "\n",
        "    # Call to getPrediction function with a list of reviews and logistic regression model\n",
        "    predict = getPrediction(new_data, model_logreg)\n",
        "    print(\"Prediction using Logistic Regression model:\")\n",
        "    predict.show()\n",
        "\n",
        "    # Store this result in a single file on the cluster\n",
        "    predict.coalesce(1).write.format(\"csv\").option(\"header\",True).save(sys.argv[1]+'.logreg_prediction')\n",
        "\n",
        "    ##################### Prediction using SVM model model ####################\n",
        "\n",
        "    # Call to getPrediction function with a list of reviews and SVM model\n",
        "    predict = getPrediction(new_data, model_lsvc)\n",
        "    print(\"Prediction using SVM model:\")\n",
        "    predict.show()\n",
        "\n",
        "    # Store this result in a single file on the cluster\n",
        "    predict.coalesce(1).write.format(\"csv\").option(\"header\",True).save(sys.argv[1]+'.SVM_prediction')\n",
        "\n",
        "    # Stop spark context   \n",
        "    spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-MpJkUworKH"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "SX2j064mo09A",
        "outputId": "c61b6bb6-a2e8-46d5-822e-3dae03f6133c"
      },
      "outputs": [],
      "source": [
        "# Set your file path here \n",
        "data_file = \"/Users/munibasiddiqi/Desktop/BUCS777/Homework_Assignments/Term_Project/IMDB_Dataset.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fSd3R2YKzIar"
      },
      "outputs": [],
      "source": [
        "# Upload data into a dataframe\n",
        "spark_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"escape\",\"\\\"\").option(\"multiLine\",\"true\").load(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Exxy0Nb1k73",
        "outputId": "830c4a5e-6197-4a4f-84eb-0cd43f82fde8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+\n",
            "|              review|sentiment|\n",
            "+--------------------+---------+\n",
            "|One of the other ...| positive|\n",
            "|A wonderful littl...| positive|\n",
            "|I thought this wa...| positive|\n",
            "|Basically there's...| negative|\n",
            "|Petter Mattei's \"...| positive|\n",
            "|Probably my all-t...| positive|\n",
            "|I sure would like...| positive|\n",
            "|This show was an ...| negative|\n",
            "|Encouraged by the...| negative|\n",
            "|If you like origi...| positive|\n",
            "+--------------------+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Top 10 rows\n",
        "spark_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sD4lC568SALg"
      },
      "outputs": [],
      "source": [
        "# Add label column to dataframe (Positive sentiment = 1, Negative sentiment = 0)\n",
        "df = spark_df.withColumn(\"label\", F.when(F.col(\"sentiment\")==\"positive\",1).otherwise(0)).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoJNljDKWWKv",
        "outputId": "b79ff73c-c1fe-473e-fb38-49a075c1c4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+-----+\n",
            "|              review|sentiment|label|\n",
            "+--------------------+---------+-----+\n",
            "|One of the other ...| positive|    1|\n",
            "|A wonderful littl...| positive|    1|\n",
            "|I thought this wa...| positive|    1|\n",
            "|Basically there's...| negative|    0|\n",
            "|Petter Mattei's \"...| positive|    1|\n",
            "|Probably my all-t...| positive|    1|\n",
            "|I sure would like...| positive|    1|\n",
            "|This show was an ...| negative|    0|\n",
            "|Encouraged by the...| negative|    0|\n",
            "|If you like origi...| positive|    1|\n",
            "|Phil the Alien is...| negative|    0|\n",
            "|I saw this movie ...| negative|    0|\n",
            "|So im not a big f...| negative|    0|\n",
            "|The cast played S...| negative|    0|\n",
            "|This a fantastic ...| positive|    1|\n",
            "|Kind of drawn in ...| negative|    0|\n",
            "|Some films just s...| positive|    1|\n",
            "|This movie made i...| negative|    0|\n",
            "|I remember this f...| positive|    1|\n",
            "|An awful film! It...| negative|    0|\n",
            "+--------------------+---------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uLvIBLQJXPdQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into training and test set\n",
        "print(\"Training Data\")\n",
        "df_train, df_test = df.randomSplit(weights=[0.7, 0.3], seed=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vtsZzi1XwR1",
        "outputId": "151430d9-f91a-4cf6-ffc8-c294a60b6fc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34993"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count\n",
        "df_train.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBDuF3RAX0lE",
        "outputId": "5c385397-c605-4935-8769-a71b8c81ca69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15007"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H-pp97eX6ua",
        "outputId": "9f14c7fc-beca-42b6-efaf-1e8036e5ebdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    1|17450|\n",
            "|    0|17543|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check if the dataset is balanced or imbalanced\n",
        "df_train.groupby(\"label\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnzkWWUhYJP2",
        "outputId": "fa56e9c9-0117-4173-fd17-112a84fd3fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    1| 7550|\n",
            "|    0| 7457|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_test.groupby(\"label\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7550"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.filter(\"label == 1\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdYwKndDbbSt"
      },
      "source": [
        "# Tokenize - Converts to lowercase and split text on non-word character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LqPQVL5YSwz",
        "outputId": "7d0d23a5-3d41-49d6-d5a4-33e75f933a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+------+\n",
            "|              review|               words|tokens|\n",
            "+--------------------+--------------------+------+\n",
            "|\\b\\b\\b\\bA Turkish...|[a, turkish, bath...|   635|\n",
            "|!!! Spoiler alert...|[spoiler, alert, ...|   177|\n",
            "+--------------------+--------------------+------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+--------------------+---------+-----+--------------------+\n",
            "|              review|sentiment|label|               words|\n",
            "+--------------------+---------+-----+--------------------+\n",
            "|\\b\\b\\b\\bA Turkish...| positive|    1|[a, turkish, bath...|\n",
            "|!!! Spoiler alert...| negative|    0|[spoiler, alert, ...|\n",
            "|!!!! MILD SPOILER...| negative|    0|[mild, spoilers, ...|\n",
            "|!!!! POSSIBLE MIL...| negative|    0|[possible, mild, ...|\n",
            "|!!!!! OF COURSE T...| negative|    0|[of, course, ther...|\n",
            "|!!!!! POSSIBLE SP...| negative|    0|[possible, spoile...|\n",
            "|\" Domino \" has be...| positive|    1|[domino, has, bee...|\n",
            "|\" I have wrestled...| positive|    1|[i, have, wrestle...|\n",
            "|\" It had to be Yo...| negative|    0|[it, had, to, be,...|\n",
            "|\" While sporadica...| negative|    0|[while, sporadica...|\n",
            "|\"... the beat is ...| positive|    1|[the, beat, is, t...|\n",
            "|\"54\" is a film ba...| negative|    0|[54, is, a, film,...|\n",
            "|\"8 SIMPLE RULES.....| positive|    1|[8, simple, rules...|\n",
            "|\"9/11,\" hosted by...| positive|    1|[9, 11, hosted, b...|\n",
            "|\"A Classic is som...| positive|    1|[a, classic, is, ...|\n",
            "|\"A Cry in the Dar...| positive|    1|[a, cry, in, the,...|\n",
            "|\"A Damsel in Dist...| negative|    0|[a, damsel, in, d...|\n",
            "|\"A Family Affair\"...| positive|    1|[a, family, affai...|\n",
            "|\"A Gentleman's Ga...| negative|    0|[a, gentleman, s,...|\n",
            "|\"A Girl's Folly\" ...| positive|    1|[a, girl, s, foll...|\n",
            "+--------------------+---------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "regexTokenizer = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "df_regexTokenized = regexTokenizer.transform(df_train)\n",
        "\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "df_regexTokenized.select(\"review\", \"words\") \\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(2)\n",
        "\n",
        "#regexTokenized.select(\"words\").show(truncate=0)\n",
        "df_regexTokenized.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnisRMsybkTY"
      },
      "source": [
        "# Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbnFwgXRbvDR",
        "outputId": "bdebf1f7-adb7-4ec4-8c1a-8cdfe5d35d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+-----+--------------------+--------------------+\n",
            "|              review|sentiment|label|               words|            filtered|\n",
            "+--------------------+---------+-----+--------------------+--------------------+\n",
            "|\\b\\b\\b\\bA Turkish...| positive|    1|[a, turkish, bath...|[turkish, bath, s...|\n",
            "|!!! Spoiler alert...| negative|    0|[spoiler, alert, ...|[spoiler, alert, ...|\n",
            "+--------------------+---------+-----+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|filtered                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[turkish, bath, sequence, film, noir, located, new, york, 50, must, hint, something, something, curiously, previous, comments, one, pointed, seems, essential, understanding, movie, br, br, turkish, baths, sequence, back, street, night, entrance, sleazy, sauna, scalise, wrapped, sheet, getting, thighs, massaged, steve, masseur, young, rough, boxer, beefcake, type, another, guy, bodyguard, finishes, dressing, dixon, obviously, hates, sees, gets, rough, right, away, know, reputation, roughing, suspects, good, cop, getting, control, easy, hates, much, br, br, hates, part, inherited, father, dark, side, lead, right, end, sidewalk, gutter, dark, side, lurked, within, closet, remember, whenever, dixon, meets, scalise, 3, times, guy, lying, bed, men, around, company, irony, girls, poster, pinned, wall, near, bed, br, br, scalise, acts, funny, affected, manners, cranking, neck, arrogantly, defiant, shoving, inhalator, poppers, nostrils, time, talks, dixon, dixon, vengeance, bent, pinning, scalise, seems, understand, never, saw, man, full, hate, consider, almost, humorous, way, came, alone, four, years, jumping, somebody, special, br, br, scalise, someone, special, indeed, direct, inheritor, dixon, father, father, liked, father, set, business, stands, dixon, criminal, brother, dark, side, incarnate, top, prefers, company, men, dixon, knows, well, killed, paine, one, playmates, playmates, notice, time, meet, dixon, manhandles, scalise, picks, address, book, jacket, slaps, face, punches, scalise, warn, touch, dixon, homophobia, obvious, put, different, unexpressed, homosexuality, dixon, aka, dixon, kid, son, thief, reaction, decided, become, cop, good, one, something, criminal, dark, side, violent, copper, murderer, liar, besides, married, brings, dizzy, blonde, familiar, eat, place, every, nothing, else, waitress, scoffing, says, doesn, know, make, love, woman, dixon, deep, feeling, guilt, hates, reasons, hood, mobster, like, old, man, blood, tell, finally, order, achieve, redemption, dixon, decides, sacrifice, gets, alter, ego, scalise, kill, free, guilt, free, girl, father, br, br, end, movie, brings, us, back, opening, sequence, scalise, pushed, gutter, dixon, deserves, right, walk, sidewalk, wins, love, dame, straight, last, br, br, unspoken, theme, movie, well, man, order, cover, repressed, feelings, wants, experience, woman, love, jean, douchet, br, br, notes, owe, lot, film, commentary, jean, douchet, french, dvd, edited, carlottai]|\n",
            "|[spoiler, alert, br, br, point, though, didn, think, film, ending, spoil, started, watching, middle, matt, gotten, sarah, body, became, fascinated, bizarreness, plot, even, channel, 5, movie, couldn, possibly, see, matt, wld, end, happy, fiancee, one, stage, looked, like, gonna, get, best, friend, surely, icky, wrong, whole, oggi, oggi, oggi, thing, work, touching, buddy, buddy, catchphrase, tis, ridiculous, going, surely, come, back, life, yet, live, woman, film, got, ending, explaining, anything, cross, wasted, whole, hour, life, reason, one, funniest, films, ve, ever, seen, swings, roundabouts]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "df_stopwords_removed = remover.transform(df_regexTokenized)\n",
        "df_stopwords_removed.show(2)\n",
        "df_stopwords_removed.select(\"filtered\").show(2, truncate=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+-----+--------------------+--------------------+--------------------+\n",
            "|              review|sentiment|label|               words|            filtered|               token|\n",
            "+--------------------+---------+-----+--------------------+--------------------+--------------------+\n",
            "|\\b\\b\\b\\bA Turkish...| positive|    1|[a, turkish, bath...|[turkish, bath, s...|[turkish, bath, s...|\n",
            "|!!! Spoiler alert...| negative|    0|[spoiler, alert, ...|[spoiler, alert, ...|[spoiler, alert, ...|\n",
            "+--------------------+---------+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|token                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[turkish, bath, sequence, film, noir, located, new, york, 50, must, hint, something, something, curiously, previous, comments, one, pointed, seems, essential, understanding, movie, turkish, baths, sequence, back, street, night, entrance, sleazy, sauna, scalise, wrapped, sheet, getting, thighs, massaged, steve, masseur, young, rough, boxer, beefcake, type, another, guy, bodyguard, finishes, dressing, dixon, obviously, hates, sees, gets, rough, right, away, know, reputation, roughing, suspects, good, cop, getting, control, easy, hates, much, hates, part, inherited, father, dark, side, lead, right, end, sidewalk, gutter, dark, side, lurked, within, closet, remember, whenever, dixon, meets, scalise, 3, times, guy, lying, bed, men, around, company, irony, girls, poster, pinned, wall, near, bed, scalise, acts, funny, affected, manners, cranking, neck, arrogantly, defiant, shoving, inhalator, poppers, nostrils, time, talks, dixon, dixon, vengeance, bent, pinning, scalise, seems, understand, never, saw, man, full, hate, consider, almost, humorous, way, came, alone, four, years, jumping, somebody, special, scalise, someone, special, indeed, direct, inheritor, dixon, father, father, liked, father, set, business, stands, dixon, criminal, brother, dark, side, incarnate, top, prefers, company, men, dixon, knows, well, killed, paine, one, playmates, playmates, notice, time, meet, dixon, manhandles, scalise, picks, address, book, jacket, slaps, face, punches, scalise, warn, touch, dixon, homophobia, obvious, put, different, unexpressed, homosexuality, dixon, aka, dixon, kid, son, thief, reaction, decided, become, cop, good, one, something, criminal, dark, side, violent, copper, murderer, liar, besides, married, brings, dizzy, blonde, familiar, eat, place, every, nothing, else, waitress, scoffing, says, doesn, know, make, love, woman, dixon, deep, feeling, guilt, hates, reasons, hood, mobster, like, old, man, blood, tell, finally, order, achieve, redemption, dixon, decides, sacrifice, gets, alter, ego, scalise, kill, free, guilt, free, girl, father, end, movie, brings, us, back, opening, sequence, scalise, pushed, gutter, dixon, deserves, right, walk, sidewalk, wins, love, dame, straight, last, unspoken, theme, movie, well, man, order, cover, repressed, feelings, wants, experience, woman, love, jean, douchet, notes, owe, lot, film, commentary, jean, douchet, french, dvd, edited, carlottai]|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "remover = StopWordsRemover(inputCol=\"filtered\", outputCol=\"token\",stopWords=[\"br\", 'm', 've', 're', 'll', 'd'])\n",
        "df_stopwords_removed2 = remover.transform(df_stopwords_removed)\n",
        "df_stopwords_removed2.show(2)\n",
        "df_stopwords_removed2.select(\"token\").show(1, truncate=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|               token|tokens|\n",
            "+--------------------+------+\n",
            "|[turkish, bath, s...|   313|\n",
            "|[spoiler, alert, ...|    81|\n",
            "+--------------------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_stopwords_removed2.select(\"token\") \\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"token\"))).show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb7mYOe1cdKP"
      },
      "source": [
        "# Extracts a vocabulary from document collections and generates a CountVectorizerModel.During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSDbk4BVcUBo",
        "outputId": "8ac61d22-5b59-4cba-f715-7678cc6411e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|              review|sentiment|label|               words|            filtered|               token|         rawFeatures|\n",
            "+--------------------+---------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|\\b\\b\\b\\bA Turkish...| positive|    1|[a, turkish, bath...|[turkish, bath, s...|[turkish, bath, s...|(5000,[0,1,2,3,4,...|\n",
            "|!!! Spoiler alert...| negative|    0|[spoiler, alert, ...|[spoiler, alert, ...|[spoiler, alert, ...|(5000,[0,1,2,3,6,...|\n",
            "+--------------------+---------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|rawFeatures                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(5000,[0,1,2,3,4,5,10,11,19,20,31,32,37,40,42,47,49,55,56,59,60,63,66,70,71,72,73,74,82,83,84,96,97,103,106,109,111,112,128,131,133,135,151,158,159,160,162,164,174,187,200,210,212,218,228,229,251,262,267,269,270,273,275,300,313,319,330,342,363,365,369,388,399,402,411,412,413,426,430,431,441,457,469,488,503,515,543,559,577,582,584,600,608,614,615,628,663,712,718,740,742,745,749,770,771,782,792,799,831,839,847,866,904,939,942,943,969,983,987,1021,1026,1062,1105,1191,1207,1245,1249,1251,1261,1290,1297,1306,1310,1379,1530,1552,1617,1708,1814,1831,1900,1916,1952,2007,2048,2295,2306,2450,2560,2562,2684,2686,2691,2804,2932,2939,2995,3054,3077,3106,3114,3158,3183,3210,3338,3409,3417,3491,3509,3627,3657,3730,3944,3960,4187,4219,4277,4617,4633,4970],[3.0,2.0,3.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,3.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0])|\n",
            "|(5000,[0,1,2,3,6,9,12,23,24,29,30,35,36,39,42,49,51,52,57,58,62,67,88,102,107,115,126,135,136,157,184,235,291,301,314,321,409,450,510,513,516,518,532,744,778,790,857,1061,1204,1227,1239,1487,1491,1658,1787,2078,2080,2266,2546,4062,4362,4963],[1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# fit a CountVectorizerModel from the corpus.\n",
        "cv = CountVectorizer(inputCol=\"token\", outputCol=\"rawFeatures\", vocabSize=5000)\n",
        "cv_model = cv.fit(df_stopwords_removed2)\n",
        "\n",
        "df_featurizedData = cv_model.transform(df_stopwords_removed2)\n",
        "\n",
        "df_featurizedData.show(2)\n",
        "df_featurizedData.select(\"rawFeatures\").show(2,truncate=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UYenIl3c6hB"
      },
      "source": [
        "# Top 10 vocabulary words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_QzL9V4c_jA",
        "outputId": "b4b4563e-ea40-4d0f-cf23-40bc14591a29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['movie',\n",
              " 'film',\n",
              " 'one',\n",
              " 'like',\n",
              " 'good',\n",
              " 'time',\n",
              " 'even',\n",
              " 'story',\n",
              " 'really',\n",
              " 'see']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_model.vocabulary[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6A9unP0dNrB",
        "outputId": "12a0edf1-bb24-480e-9529-c6edef369bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+\n",
            "|Top 10 vocabulary words|\n",
            "+-----------------------+\n",
            "|movie                  |\n",
            "|film                   |\n",
            "|one                    |\n",
            "|like                   |\n",
            "|good                   |\n",
            "|time                   |\n",
            "|even                   |\n",
            "|story                  |\n",
            "|really                 |\n",
            "|see                    |\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_top10 = spark.createDataFrame(cv_model.vocabulary[0:10], StringType())\n",
        "df_top10.withColumnRenamed(\"value\", \"Top 10 vocabulary words\").show(truncate=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['movie',\n",
              " 'film',\n",
              " 'one',\n",
              " 'like',\n",
              " 'good',\n",
              " 'time',\n",
              " 'even',\n",
              " 'story',\n",
              " 'really',\n",
              " 'see']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv = CountVectorizer(inputCol=\"token\", outputCol=\"rawFeatures\", vocabSize=5000, minDF=100, maxDF= 0.75)\n",
        "cv_model = cv.fit(df_stopwords_removed2)\n",
        "\n",
        "cv_model.vocabulary[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZCsmNm3d2nV"
      },
      "source": [
        "# IDF: IDF is an Estimator which is fit on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each feature. Intuitively, it down-weights features which appear frequently in a corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW6kyNX6djpB",
        "outputId": "a842174c-260d-485f-c862-43ecc9a2d80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----+--------------------+\n",
            "|sentiment|label|            features|\n",
            "+---------+-----+--------------------+\n",
            "| positive|    1|(5000,[0,1,2,3,4,...|\n",
            "| negative|    0|(5000,[0,1,2,3,6,...|\n",
            "| negative|    0|(5000,[0,3,4,7,11...|\n",
            "| negative|    0|(5000,[1,3,5,6,17...|\n",
            "| negative|    0|(5000,[1,2,9,11,1...|\n",
            "| negative|    0|(5000,[0,1,3,7,15...|\n",
            "| positive|    1|(5000,[0,1,3,7,8,...|\n",
            "| positive|    1|(5000,[1,2,3,5,7,...|\n",
            "| negative|    0|(5000,[0,8,16,18,...|\n",
            "| negative|    0|(5000,[1,3,10,25,...|\n",
            "+---------+-----+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(df_featurizedData)\n",
        "df_rescaledData = idfModel.transform(df_featurizedData)\n",
        "df_rescaledData.cache()\n",
        "df_rescaledData.select(\"sentiment\", \"label\", \"features\").show(10)\n",
        "#df_rescaledData.show(2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lcALwcIenKa"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dbyhmJMWePCd"
      },
      "outputs": [],
      "source": [
        "#tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "# Converts text to lowercase and split text on non-word character\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "    \n",
        "# Remove stopwords\n",
        "remover = StopWordsRemover(inputCol = regexTokenizer.getOutputCol(), outputCol=\"filtered\")\n",
        "\n",
        "# Remove stopWords=[\"br\", 'm', 've', 're', 'll', 'd']\n",
        "remover2 = StopWordsRemover(inputCol= remover.getOutputCol(), outputCol=\"token\",stopWords=[\"br\", 'm', 've', 're', 'll', 'd'])\n",
        "\n",
        "# Extracts a vocabulary from document collections and generates a CountVectorizerModel\n",
        "# During the fitting process, CountVectorizer will select the top vocabSize words ordered \n",
        "# by term frequency across the corpus.\n",
        "countVectorizer = CountVectorizer(inputCol= remover2.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=5000)\n",
        "    \n",
        "# The IDF Model takes feature vectors and scales each feature. \n",
        "# Intuitively, it down-weights features which appear frequently in a corpus\n",
        "idf = IDF(inputCol= countVectorizer.getOutputCol(), outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR1kEXACeN-U"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOk6_wiTe7Np",
        "outputId": "66306da2-08a9-4a02-e6e9-ca81b296c852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coefficients: [-0.07463579  0.01510059 -0.09580708  0.06750367  0.29349161  0.26400688\n",
            " -0.18612552  0.0903201  -0.04318012  0.31925709]\n",
            "Intercept: -0.1429045478912475\n"
          ]
        }
      ],
      "source": [
        "# LogisticRegression classifier\n",
        "classifier_logreg = LogisticRegression(maxIter=20)\n",
        "\n",
        "# Chain indexers and classifier_logreg in a Pipeline\n",
        "pipeline_logreg = Pipeline(stages=[regexTokenizer, remover, remover2, countVectorizer, idf, classifier_logreg])\n",
        "# Train model. \n",
        "model_logreg = pipeline_logreg.fit(df_train)\n",
        "\n",
        "# Print the coefficients and intercept for linear SVC\n",
        "print(\"Coefficients: \" + str(model_logreg.stages[5].coefficients[:10]))\n",
        "print(\"Intercept: \" + str(model_logreg.stages[5].intercept))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['movie', 'film', 'one', 'like', 'good', 'time', 'even', 'story', 'really', 'see', 'well', 'much', 'get', 'bad', 'people', 'great', 'also', 'first', 'made', 'make', 'way', 'movies', 'characters', 'think', 'films', 'watch', 'character', 'two', 'many', 'seen', 'life', 'love', 'never', 'acting', 'show', 'plot', 'best', 'know', 'little', 'ever', 'man', 'better', 'end', 'scene', 'still', 'say', 'scenes', 'something', 'go', 'back', 'real', 'watching', 'thing', 'director', 'actors', 'years', 'doesn', 'though', 'didn', 'funny', 'another', '10', 'work', 'old', 'actually', 'look', 'nothing', 'going', 'makes', 'find', 'lot', 'new', 'every', 'part', 'us', 'things', 'world', 'cast', 'horror', 'want', 'quite', 'pretty', 'around', 'seems', 'young', 'take', 'however', 'long', 'got', 'big', 'enough', 'thought', 'fact', 'give', 'series', 'comedy', 'right', 'must', 'music', 'action', 'may', 'without', 'come', 'guy', 'always', 'isn', 'saw', 'point', 'original', 'gets', 'done', 'times', 'almost', 'role', 'interesting', 'whole', 'least', 'far', 'bit', 'family', 'script', 'minutes', '2', 'might', 'making', 'feel', 'anything', 'since', 'last', 'probably', 'tv', 'away', 'performance', 'girl', 'kind', 'woman', 'yet', 'day', 'worst', 'anyone', 'sure', 'fun', 'rather', 'hard', 'looking', 'screen', 'played', 'found', 'believe', 'especially', 'although', 'dvd', 'course', 'trying', 'comes', 'everything', 'maybe', 'ending', 'place', 'set', 'different', 'shows', 'put', 'let', 'book', 'worth', 'goes', 'three', 'actor', 'sense', 'american', 'looks', 'money', 'main', 'someone', 'true', 'wasn', 'watched', 'play', 'year', 'effects', 'together', 'everyone', 'job', 'reason', 'takes', 'instead', 'night', '1', 'war', 'plays', 'audience', 'john', 'beautiful', 'half', 'said', 'high', 'later', 'shot', 'seem']\n"
          ]
        }
      ],
      "source": [
        "pipeline_logreg.getStages()\n",
        "vocabulary = model_logreg.stages[3].vocabulary\n",
        "print(vocabulary[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---------+----------+\n",
            "|label|sentiment|prediction|\n",
            "+-----+---------+----------+\n",
            "|    0| negative|       0.0|\n",
            "|    1| positive|       1.0|\n",
            "|    1| positive|       1.0|\n",
            "|    1| positive|       0.0|\n",
            "|    1| positive|       1.0|\n",
            "|    1| positive|       1.0|\n",
            "|    0| negative|       0.0|\n",
            "|    1| positive|       0.0|\n",
            "|    0| negative|       0.0|\n",
            "|    0| negative|       0.0|\n",
            "+-----+---------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make predictions.\n",
        "predictions_logreg = model_logreg.transform(df_test).cache()\n",
        "\n",
        "selected = predictions_logreg.select(\"label\", \"sentiment\", \"prediction\")\n",
        "selected.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Ys9x9Hg40c"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAXhWqQYfeR2",
        "outputId": "6e1aa51a-4f85-437d-c23a-c1470f9b4ac5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (1.0, 0.0),\n",
              " (0.0, 0.0),\n",
              " (0.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (0.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (0.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (0.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (0.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 0.0),\n",
              " (0.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (0.0, 0.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0),\n",
              " (1.0, 1.0)]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Covert dataframe to RDD for Model evaluation\n",
        "predictionAndLabels_logreg = predictions_logreg.select(\"label\",  \"prediction\").rdd.map(lambda x : (float(x[0]), float(x[1]))).cache()\n",
        "predictionAndLabels_logreg.take(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuDQGUNngINL",
        "outputId": "4117cfaf-0960-47bd-d569-87a65c224a53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/munibasiddiqi/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary statistics for Logistic regression classifier\n",
            "Precision = 0.8560264900662252\n",
            "Recall = 0.858414132022845\n",
            "F1 Score = 0.8572186484514889\n",
            "Accuracy = 0.8565336176450989\n",
            "Class 0.0 precision = 0.8570470698672389\n",
            "Class 0.0 recall = 0.8546402781492377\n",
            "Class 0.0 F1 Measure = 0.8558419819216606\n",
            "Class 1.0 precision = 0.8560264900662252\n",
            "Class 1.0 recall = 0.858414132022845\n",
            "Class 1.0 F1 Measure = 0.8572186484514889\n",
            "[[6391 1087]\n",
            " [1066 6463]]\n"
          ]
        }
      ],
      "source": [
        "# Model evaluation\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "# Instantiate metrics object\n",
        "metrics_logreg = MulticlassMetrics(predictionAndLabels_logreg)\n",
        "\n",
        "# Overall statistics\n",
        "print(\"Summary statistics for Logistic regression classifier\")\n",
        "print(\"Precision = %s\" % metrics_logreg.precision(1.0))\n",
        "print(\"Recall = %s\" % metrics_logreg.recall(1.0))\n",
        "print(\"F1 Score = %s\" % metrics_logreg.fMeasure(1.0))\n",
        "print(\"Accuracy = %s\" % metrics_logreg.accuracy)\n",
        "\n",
        "# Statistics by class\n",
        "#labels = data.map(lambda lp: lp.label).distinct().collect()\n",
        "labels = [0.0, 1.0]\n",
        "for label in sorted(labels):\n",
        "    print(\"Class %s precision = %s\" % (label, metrics_logreg.precision(label)))\n",
        "    print(\"Class %s recall = %s\" % (label, metrics_logreg.recall(label)))\n",
        "    print(\"Class %s F1 Measure = %s\" % (label, metrics_logreg.fMeasure(label, beta=1.0)))\n",
        "\n",
        "print(metrics_logreg.confusionMatrix().toArray().astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRQeBwHwhPoO"
      },
      "source": [
        "# SVM\n",
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "s3GR8W7FhOP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coefficients: [-0.02944016  0.0002544   0.00930135 -0.00202794  0.08904828  0.04703349\n",
            " -0.07170383  0.00976485 -0.00446598  0.10477262]\n",
            "Intercept: -0.07833200139903072\n"
          ]
        }
      ],
      "source": [
        "# SVM classifier\n",
        "classifier_lsvc = LinearSVC(maxIter=20)\n",
        "\n",
        "# Fit the model\n",
        "#lsvcModel = classifier.fit(df_train)\n",
        "\n",
        "# Chain indexers and classifier_lsvc in a Pipeline\n",
        "pipeline_lsvc = Pipeline(stages=[regexTokenizer, remover, remover2, countVectorizer, idf, classifier_lsvc])\n",
        "\n",
        "# Train model. \n",
        "model_lsvc = pipeline_lsvc.fit(df_train)\n",
        "\n",
        "# Print the coefficients and intercept for linear SVC\n",
        "print(\"Coefficients: \" + str(model_lsvc.stages[5].coefficients[:10]))\n",
        "print(\"Intercept: \" + str(model_lsvc.stages[5].intercept))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['movie', 'film', 'one', 'like', 'good', 'time', 'even', 'story', 'really', 'see', 'well', 'much', 'get', 'bad', 'people', 'great', 'also', 'first', 'made', 'make', 'way', 'movies', 'characters', 'think', 'films', 'watch', 'character', 'two', 'many', 'seen', 'life', 'love', 'never', 'acting', 'show', 'plot', 'best', 'know', 'little', 'ever', 'man', 'better', 'end', 'scene', 'still', 'say', 'scenes', 'something', 'go', 'back', 'real', 'watching', 'thing', 'director', 'actors', 'years', 'doesn', 'though', 'didn', 'funny', 'another', '10', 'work', 'old', 'actually', 'look', 'nothing', 'going', 'makes', 'find', 'lot', 'new', 'every', 'part', 'us', 'things', 'world', 'cast', 'horror', 'want', 'quite', 'pretty', 'around', 'seems', 'young', 'take', 'however', 'long', 'got', 'big', 'enough', 'thought', 'fact', 'give', 'series', 'comedy', 'right', 'must', 'music', 'action', 'may', 'without', 'come', 'guy', 'always', 'isn', 'saw', 'point', 'original', 'gets', 'done', 'times', 'almost', 'role', 'interesting', 'whole', 'least', 'far', 'bit', 'family', 'script', 'minutes', '2', 'might', 'making', 'feel', 'anything', 'since', 'last', 'probably', 'tv', 'away', 'performance', 'girl', 'kind', 'woman', 'yet', 'day', 'worst', 'anyone', 'sure', 'fun', 'rather', 'hard', 'looking', 'screen', 'played', 'found', 'believe', 'especially', 'although', 'dvd', 'course', 'trying', 'comes', 'everything', 'maybe', 'ending', 'place', 'set', 'different', 'shows', 'put', 'let', 'book', 'worth', 'goes', 'three', 'actor', 'sense', 'american', 'looks', 'money', 'main', 'someone', 'true', 'wasn', 'watched', 'play', 'year', 'effects', 'together', 'everyone', 'job', 'reason', 'takes', 'instead', 'night', '1', 'war', 'plays', 'audience', 'john', 'beautiful', 'half', 'said', 'high', 'later', 'shot', 'seem']\n"
          ]
        }
      ],
      "source": [
        "pipeline_lsvc.getStages()\n",
        "vocabulary = model_lsvc.stages[3].vocabulary\n",
        "print(vocabulary[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLMGxJzsiRSs",
        "outputId": "c9a5c727-0272-46e5-9b82-3cc835ea5ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---------+----------+\n",
            "|label|sentiment|prediction|\n",
            "+-----+---------+----------+\n",
            "|    0| negative|       0.0|\n",
            "|    1| positive|       1.0|\n",
            "|    1| positive|       1.0|\n",
            "|    1| positive|       0.0|\n",
            "|    1| positive|       1.0|\n",
            "|    1| positive|       1.0|\n",
            "|    0| negative|       0.0|\n",
            "|    1| positive|       0.0|\n",
            "|    0| negative|       0.0|\n",
            "|    0| negative|       0.0|\n",
            "+-----+---------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make predictions.\n",
        "predictions_lsvc = model_lsvc.transform(df_test).cache()\n",
        "\n",
        "selected = predictions_lsvc.select(\"label\", \"sentiment\", \"prediction\")\n",
        "selected.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJWX_Qlmibjf"
      },
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL9f7Tdwifyb",
        "outputId": "44cf53c8-a8c0-4d73-ee22-0612d9636248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary statistics for SVM classier\n",
            "Precision = 0.8794701986754967\n",
            "Recall = 0.8825093035619351\n",
            "F1 Score = 0.8809871301578877\n",
            "Accuracy = 0.8804557872992603\n",
            "Class 0.0 precision = 0.8814536676947834\n",
            "Class 0.0 recall = 0.8783910196445276\n",
            "Class 0.0 F1 Measure = 0.8799196787148594\n",
            "Class 1.0 precision = 0.8794701986754967\n",
            "Class 1.0 recall = 0.8825093035619351\n",
            "Class 1.0 F1 Measure = 0.8809871301578877\n",
            "[[6573  910]\n",
            " [ 884 6640]]\n"
          ]
        }
      ],
      "source": [
        "# Model evaluation\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Covert dataframe to RDD for Model evaluation\n",
        "predictionAndLabels_lsvc = predictions_lsvc.select(\"label\",  \"prediction\").rdd.map(lambda x : (float(x[0]), float(x[1]))).cache()\n",
        "predictionAndLabels_lsvc.take(40)\n",
        "\n",
        "# Instantiate metrics object\n",
        "metrics_lsvc = MulticlassMetrics(predictionAndLabels_lsvc)\n",
        "\n",
        "# Overall statistics\n",
        "print(\"Summary statistics for SVM classier\")\n",
        "print(\"Precision = %s\" % metrics_lsvc.precision(1.0))\n",
        "print(\"Recall = %s\" % metrics_lsvc.recall(1.0))\n",
        "print(\"F1 Score = %s\" % metrics_lsvc.fMeasure(1.0))\n",
        "print(\"Accuracy = %s\" % metrics_lsvc.accuracy)\n",
        "\n",
        "# Statistics by class\n",
        "#labels = data.map(lambda lp: lp.label).distinct().collect()\n",
        "labels = [0.0, 1.0]\n",
        "for label in sorted(labels):\n",
        "    print(\"Class %s precision = %s\" % (label, metrics_lsvc.precision(label)))\n",
        "    print(\"Class %s recall = %s\" % (label, metrics_lsvc.recall(label)))\n",
        "    print(\"Class %s F1 Measure = %s\" % (label, metrics_lsvc.fMeasure(label, beta=1.0)))\n",
        "\n",
        "print(metrics_lsvc.confusionMatrix().toArray().astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THS0FLogkDqX"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "IjcybS8jkB3f"
      },
      "outputs": [],
      "source": [
        "# Converts text to lowercase and split text on non-word character\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "    \n",
        "# Remove stopwords\n",
        "remover = StopWordsRemover(inputCol = regexTokenizer.getOutputCol(), outputCol=\"filtered\")\n",
        "\n",
        "# Remove stopWords=[\"br\", 'm', 've', 're', 'll', 'd']\n",
        "remover2 = StopWordsRemover(inputCol= remover.getOutputCol(), outputCol=\"token\",stopWords=[\"br\", 'm', 've', 're', 'll', 'd'])\n",
        "\n",
        "# Extracts a vocabulary from document collections and generates a CountVectorizerModel\n",
        "# During the fitting process, CountVectorizer will select the top vocabSize words ordered \n",
        "# by term frequency across the corpus.\n",
        "countVectorizer = CountVectorizer(inputCol= remover2.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=5000)\n",
        "    \n",
        "# The IDF Model takes feature vectors and scales each feature. \n",
        "# Intuitively, it down-weights features which appear frequently in a corpus\n",
        "idf_features = IDF(inputCol= countVectorizer.getOutputCol(), outputCol=\"featuresIDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chi-Squared feature selection. It operates on labeled data with categorical features. \n",
        "# ChiSqSelector uses the Chi-Squared test of independence to decide which features to choose. \n",
        "selector = ChiSqSelector(numTopFeatures=500, featuresCol=idf_features.getOutputCol(),\n",
        "                         outputCol=\"features\", labelCol=\"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhR7wrqzlHfb"
      },
      "source": [
        "## Logistic Regression - Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR1crayskPj_",
        "outputId": "13235cbe-1eff-4be8-d491-793b58e97a41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier output with top 500 features selected using ChiSqSelector \n",
            "Summary statistics for Logistic Regression classifier with feature reduction\n",
            "Precision = 0.8882119205298014\n",
            "Recall = 0.8742015382609829\n",
            "F1 Score = 0.8811510413244859\n",
            "Accuracy = 0.8794562537482509\n",
            "Class 0.0 precision = 0.8705913906396674\n",
            "Class 0.0 recall = 0.8849509269356598\n",
            "Class 0.0 F1 Measure = 0.8777124315554655\n",
            "Class 1.0 precision = 0.8882119205298014\n",
            "Class 1.0 recall = 0.8742015382609829\n",
            "Class 1.0 F1 Measure = 0.8811510413244859\n",
            "[[6492  844]\n",
            " [ 965 6706]]\n"
          ]
        }
      ],
      "source": [
        "classifier_logreg = LogisticRegression(maxIter=20)\n",
        "\n",
        "print(\"Logistic Regression Classifier output with top %d features selected using ChiSqSelector \" % selector.getNumTopFeatures())\n",
        "\n",
        "# Chain indexers and classifier_logreg in a Pipeline\n",
        "pipeline_logreg_feature_selection = Pipeline(stages=[regexTokenizer, remover, remover2, countVectorizer, idf_features, selector, classifier_logreg])\n",
        "\n",
        "# Train model. \n",
        "model_logreg_feature_selection = pipeline_logreg_feature_selection.fit(df_train)\n",
        "\n",
        "# Make predictions.\n",
        "predictions_logreg_feature_selection = model_logreg_feature_selection.transform(df_test).cache()\n",
        "\n",
        "#selected = predictions_logreg.select(\"label\",\"sentiment\" , \"probability\", \"prediction\")\n",
        "#selected.show(2)\n",
        "\n",
        "predictionAndLabels_logreg_feature_selection = predictions_logreg_feature_selection.select(\"label\",  \"prediction\").rdd.map(lambda x : (float(x[0]), float(x[1]))).cache()\n",
        "#predictionAndLabels_logreg.take(40)\n",
        "\n",
        "# Model evaluation\n",
        "\n",
        "# Instantiate metrics object\n",
        "metrics_logreg_feature_selection = MulticlassMetrics(predictionAndLabels_logreg_feature_selection)\n",
        "\n",
        "# Overall statistics\n",
        "print(\"Summary statistics for Logistic Regression classifier with feature reduction\")\n",
        "print(\"Precision = %s\" % metrics_logreg_feature_selection.precision(1.0))\n",
        "print(\"Recall = %s\" % metrics_logreg_feature_selection.recall(1.0))\n",
        "print(\"F1 Score = %s\" % metrics_logreg_feature_selection.fMeasure(1.0))\n",
        "print(\"Accuracy = %s\" % metrics_logreg_feature_selection.accuracy)\n",
        "\n",
        "# Statistics by class\n",
        "#labels = data.map(lambda lp: lp.label).distinct().collect()\n",
        "labels = [0.0, 1.0]\n",
        "for label in sorted(labels):\n",
        "    print(\"Class %s precision = %s\" % (label, metrics_logreg_feature_selection.precision(label)))\n",
        "    print(\"Class %s recall = %s\" % (label, metrics_logreg_feature_selection.recall(label)))\n",
        "    print(\"Class %s F1 Measure = %s\" % (label, metrics_logreg_feature_selection.fMeasure(label, beta=1.0)))\n",
        "\n",
        "print(metrics_logreg_feature_selection.confusionMatrix().toArray().astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcvsxfKKlQyR"
      },
      "source": [
        "## SVM - Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aBBPBnflSaJ",
        "outputId": "53a87ae1-ebac-4f8d-f03d-5af56c24355f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM output with top 500 features selected using ChiSqSelector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/munibasiddiqi/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary statistics for SVM classifier with feature reduction\n",
            "Precision = 0.8921854304635761\n",
            "Recall = 0.8710720289667658\n",
            "F1 Score = 0.8815023228423738\n",
            "Accuracy = 0.8793229826081163\n",
            "Class 0.0 precision = 0.8663001206919673\n",
            "Class 0.0 recall = 0.8880945834478966\n",
            "Class 0.0 F1 Measure = 0.8770619781413347\n",
            "Class 1.0 precision = 0.8921854304635761\n",
            "Class 1.0 recall = 0.8710720289667658\n",
            "Class 1.0 F1 Measure = 0.8815023228423738\n",
            "[[6460  814]\n",
            " [ 997 6736]]\n"
          ]
        }
      ],
      "source": [
        "classifier_lsvc = LinearSVC(maxIter=20)\n",
        "\n",
        "print(\"SVM output with top %d features selected using ChiSqSelector\" % selector.getNumTopFeatures())\n",
        "\n",
        "# Chain indexers and classifier_logreg in a Pipeline\n",
        "pipeline_lsvc__feature_selection = Pipeline(stages=[regexTokenizer, remover, remover2, countVectorizer, idf_features, selector, classifier_lsvc])\n",
        "\n",
        "# Train model. \n",
        "model_lsvc_feature_selection = pipeline_lsvc__feature_selection.fit(df_train)\n",
        "\n",
        "# Make predictions.\n",
        "predictions_lsvc__feature_selection = model_lsvc_feature_selection.transform(df_test).cache()\n",
        "\n",
        "#selected = predictions_lsvc.select(\"label\", \"sentiment\", \"prediction\")\n",
        "#selected.show(10)\n",
        "predictionAndLabels_lsvc_feature_selection = predictions_lsvc__feature_selection.select(\"label\",  \"prediction\").rdd.map(lambda x : (float(x[0]), float(x[1]))).cache()\n",
        "#predictionAndLabels_lsvc.take(40)\n",
        "\n",
        "# Model evaluation\n",
        "\n",
        "# Instantiate metrics object\n",
        "metrics_lsvc_feature_selection = MulticlassMetrics(predictionAndLabels_lsvc_feature_selection)\n",
        "\n",
        "# Overall statistics\n",
        "print(\"Summary statistics for SVM classifier with feature reduction\")\n",
        "print(\"Precision = %s\" % metrics_lsvc_feature_selection.precision(1.0))\n",
        "print(\"Recall = %s\" % metrics_lsvc_feature_selection.recall(1.0))\n",
        "print(\"F1 Score = %s\" % metrics_lsvc_feature_selection.fMeasure(1.0))\n",
        "print(\"Accuracy = %s\" % metrics_lsvc_feature_selection.accuracy)\n",
        "\n",
        "# Statistics by class\n",
        "#labels = data.map(lambda lp: lp.label).distinct().collect()\n",
        "labels = [0.0, 1.0]\n",
        "for label in sorted(labels):\n",
        "    print(\"Class %s precision = %s\" % (label, metrics_lsvc_feature_selection.precision(label)))\n",
        "    print(\"Class %s recall = %s\" % (label, metrics_lsvc_feature_selection.recall(label)))\n",
        "    print(\"Class %s F1 Measure = %s\" % (label, metrics_lsvc_feature_selection.fMeasure(label, beta=1.0)))\n",
        "\n",
        "print(metrics_lsvc_feature_selection.confusionMatrix().toArray().astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getPrediction(text, model):\n",
        "    # Check if text is one review or a list of reviews\n",
        "    if (isinstance(text, str)):\n",
        "        review = [text]\n",
        "    else:\n",
        "        review = text\n",
        "\n",
        "    # Create a dataframe of review list\n",
        "    df_new_data = spark.createDataFrame(review, StringType()) \n",
        "\n",
        "    # Rename the dataframe column\n",
        "    df_new_data = df_new_data.withColumnRenamed(\"value\", \"review\")\n",
        "\n",
        "    # Predict sentiment using the SVM model\n",
        "    predict = model.transform(df_new_data)\n",
        "    predict = predict.select(\"review\", \"prediction\")\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction Using the Four Models\n",
        "## model_lsvc, model_logreg, model_lsvc_feature_selection, model_logreg_feature_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|              review|prediction|\n",
            "+--------------------+----------+\n",
            "|Harry Potter was ...|       1.0|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model_lsvc, model_logreg, model_lsvc_features, model_logreg_features\n",
        "# Call to getPrediction function with just one review text\n",
        "predict = getPrediction('Harry Potter was a good movie.', model_lsvc)\n",
        "type(predict)\n",
        "predict.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|              review|prediction|\n",
            "+--------------------+----------+\n",
            "|Harry Potter was ...|       1.0|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model_lsvc, model_logreg, model_lsvc_features, model_logreg_features\n",
        "# Call to getPrediction function with just one review text\n",
        "predict = getPrediction('Harry Potter was a good movie.', model_lsvc_feature_selection)\n",
        "type(predict)\n",
        "predict.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|              review|prediction|\n",
            "+--------------------+----------+\n",
            "|This movie was ho...|       0.0|\n",
            "|The film really s...|       0.0|\n",
            "|What a beautiful ...|       1.0|\n",
            "|Harry Potter was ...|       0.0|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call to getPrediction function with a list of reviews\n",
        "new_data = ['This movie was horrible, plot was boring, acting was okay.',\n",
        "            'The film really sucked. I want my money back',\n",
        "            'What a beautiful movie. Great plot, great acting.',\n",
        "            'Harry Potter was a good movie.'\n",
        "            'It was horrible and great at the sametime.'\n",
        "            ]\n",
        "\n",
        "predict = getPrediction(new_data, model_logreg_feature_selection)\n",
        "predict.show(truncate=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------+----------+\n",
            "|review                                                    |prediction|\n",
            "+----------------------------------------------------------+----------+\n",
            "|This movie was horrible, plot was boring, acting was okay.|0.0       |\n",
            "|The film really sucked. I want my money back              |0.0       |\n",
            "|What a beautiful movie. Great plot, great acting.         |1.0       |\n",
            "|Harry Potter was a good movie.                            |1.0       |\n",
            "+----------------------------------------------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call to getPrediction function with a list of reviews\n",
        "new_data = ['This movie was horrible, plot was boring, acting was okay.',\n",
        "            'The film really sucked. I want my money back',\n",
        "            'What a beautiful movie. Great plot, great acting.',\n",
        "            'Harry Potter was a good movie.'\n",
        "            ]\n",
        "\n",
        "predict = getPrediction(new_data, model_logreg_feature_selection)\n",
        "predict.show(truncate=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradient-boosted tree classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary Stats\n",
            "Precision = 0.8654304635761589\n",
            "Recall = 0.7140203256474702\n",
            "F1 Score = 0.7824681156816956\n",
            "Accuracy = 0.7579129739454921\n",
            "Class 0.0 precision = 0.6490545795896473\n",
            "Class 0.0 recall = 0.8265027322404371\n",
            "Class 0.0 F1 Measure = 0.7271088409824983\n",
            "Class 1.0 precision = 0.8654304635761589\n",
            "Class 1.0 recall = 0.7140203256474702\n",
            "Class 1.0 F1 Measure = 0.7824681156816956\n",
            "[[4840 1016]\n",
            " [2617 6534]]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "classifier_gbt = GBTClassifier(maxIter=10, featuresCol=\"rawFeatures\", labelCol=\"label\", predictionCol=\"prediction\")\n",
        "\n",
        "# Chain indexers and classifier_logreg in a Pipeline\n",
        "pipeline_gbt = Pipeline(stages=[regexTokenizer, remover, remover2, countVectorizer, classifier_gbt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model_gbt = pipeline_gbt.fit(df_train)\n",
        "\n",
        "# Make predictions.\n",
        "predictions_gbt = model_gbt.transform(df_test).cache()\n",
        "\n",
        "# Select example rows to display.\n",
        "#predictions_gbt.select(\"review\", \"label\", \"prediction\").show(5)\n",
        "#selected = predictions_gbt.select(\"label\", \"prediction\")\n",
        "#selected.show(10)\n",
        "\n",
        "predictionAndLabels_gbt = predictions_gbt.select(\"label\",  \"prediction\").rdd.map(lambda x : (float(x[0]), float(x[1]))).cache()\n",
        "#predictionAndLabels_lsvc.take(40)\n",
        "\n",
        "# Model evaluation\n",
        "\n",
        "# Instantiate metrics object\n",
        "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
        "\n",
        "# Overall statistics\n",
        "print(\"Summary Stats\")\n",
        "print(\"Precision = %s\" % metrics_gbt.precision(1.0))\n",
        "print(\"Recall = %s\" % metrics_gbt.recall(1.0))\n",
        "print(\"F1 Score = %s\" % metrics_gbt.fMeasure(1.0))\n",
        "print(\"Accuracy = %s\" % metrics_gbt.accuracy)\n",
        "\n",
        "# Statistics by class\n",
        "#labels = data.map(lambda lp: lp.label).distinct().collect()\n",
        "labels = [0.0, 1.0]\n",
        "for label in sorted(labels):\n",
        "    print(\"Class %s precision = %s\" % (label, metrics_gbt.precision(label)))\n",
        "    print(\"Class %s recall = %s\" % (label, metrics_gbt.recall(label)))\n",
        "    print(\"Class %s F1 Measure = %s\" % (label, metrics_gbt.fMeasure(label, beta=1.0)))\n",
        "\n",
        "print(metrics_gbt.confusionMatrix().toArray().astype(int))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|              review|prediction|\n",
            "+--------------------+----------+\n",
            "|this movie was ho...|       0.0|\n",
            "|the film really s...|       1.0|\n",
            "|what a beautiful ...|       1.0|\n",
            "|Harry Potter was ...|       1.0|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call to getPrediction function with a list of reviews\n",
        "new_data = ['this movie was horrible, plot was boring, acting was okay.',\n",
        "            'the film really sucked. i want my money back',\n",
        "            'what a beautiful movie. great plot, great acting.',\n",
        "            'Harry Potter was a great movie'\n",
        "            ]\n",
        "\n",
        "predict = getPrediction(new_data, model_gbt)\n",
        "predict.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Term_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
